# -*- coding: utf-8 -*-
"""yoloV8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zNpd0bUamUZJSoR_Z8ytVexhkyD8wCuP
"""



"""# STEP 1: Install Required Libraries"""

print("üì¶ Installing required libraries...")

!pip install -q ultralytics kaggle roboflow opencv-python-headless pillow matplotlib gradio
!pip install -q torch torchvision

print("‚úÖ Libraries installed successfully!")

"""# STEP 2: Setup Kaggle API for Dataset Download"""

# IMPORTANT: Import the files module first
from google.colab import files
from pathlib import Path

# Upload kaggle.json
print("Please upload your kaggle.json file when prompted...")
uploaded = files.upload()

# Get the first uploaded file (regardless of exact name)
if not uploaded:
    print("‚ùå No file uploaded! Please upload kaggle.json.")
else:
    uploaded_filename = list(uploaded.keys())[0]
    print(f"‚úÖ File uploaded: {uploaded_filename}")

    # Setup Kaggle directory
    kaggle_dir = Path.home() / '.kaggle'
    kaggle_dir.mkdir(exist_ok=True)

    # Copy the uploaded file to the .kaggle directory
    import shutil
    shutil.copy(uploaded_filename, kaggle_dir / 'kaggle.json')

    # Set permissions
    !chmod 600 ~/.kaggle/kaggle.json
    print("‚úÖ Kaggle API¬†configured!")

"""# STEP 3: Download and Extract Animal Dataset"""

print("\nüì• Downloading Animal Dataset...")


!kaggle datasets download -d iamsouravbanerjee/animal-image-dataset-90-different-animals -q


print("\nüìÇ Extracting dataset...")
!unzip -q animal-image-dataset-90-different-animals.zip -d /content/animals_90


print("\nüìÅ Dataset structure:")
!ls -la /content/animals_90

"""# STEP 4: Prepare Dataset in YOLO Format with Proper Bounding Boxes"""

print("\nüîÑ Preparing dataset in YOLO format...")

import yaml
from sklearn.model_selection import train_test_split
import cv2
import numpy as np
from PIL import Image

def create_yolo_dataset_with_proper_bboxes(dataset_root, output_root):
    """
    Create YOLO format dataset with proper bounding boxes
    Uses image dimensions to create reasonable bounding boxes
    """

    dataset_root = Path(dataset_root)
    output_root = Path(output_root)


    if (dataset_root / "animals").exists():
        inner = dataset_root / "animals"
        inner2 = inner / "animals"
        if inner2.exists() and len(list(inner2.iterdir())) > 10:
            source_dir = inner2
        else:
            source_dir = inner
    else:
        raise Exception("‚ùå Dataset folder structure not recognized")

    print(f"üìÅ Using source folder: {source_dir}")

    # Create YOLO output structure
    train_img = output_root / "train" / "images"
    train_lbl = output_root / "train" / "labels"
    val_img = output_root / "val" / "images"
    val_lbl = output_root / "val" / "labels"

    for p in [train_img, train_lbl, val_img, val_lbl]:
        p.mkdir(parents=True, exist_ok=True)

    # Get class folders
    class_folders = sorted([d for d in source_dir.iterdir() if d.is_dir()])
    class_names = [d.name for d in class_folders]

    print(f"üêæ Classes detected: {len(class_names)}")

    # Process each class
    for class_id, class_name in enumerate(class_names):
        folder = source_dir / class_name
        images = list(folder.glob("*.jpg")) + list(folder.glob("*.jpeg")) + list(folder.glob("*.png"))

        if len(images) == 0:
            print(f"‚ö†Ô∏è Warning: No images found in '{class_name}' ‚Äî skipped")
            continue

        train_set, val_set = train_test_split(images, test_size=0.2, random_state=42)

        # Process training images
        for img_path in train_set:
            process_image_for_yolo(img_path, train_img, train_lbl, class_name, class_id)

        # Process validation images
        for img_path in val_set:
            process_image_for_yolo(img_path, val_img, val_lbl, class_name, class_id)

        print(f"‚úî {class_name}: {len(train_set)} train, {len(val_set)} val")

    # Write data.yaml
    data_yaml = {
        "path": str(output_root),
        "train": "train/images",
        "val": "val/images",
        "nc": len(class_names),
        "names": class_names
    }

    with open(output_root / "data.yaml", "w") as f:
        yaml.dump(data_yaml, f)

    print("üéâ Dataset preparation completed!")
    return class_names

def process_image_for_yolo(img_path, img_dir, lbl_dir, class_name, class_id):
    """Process single image and create YOLO format annotations"""

    # Copy image
    dst_img = img_dir / f"{class_name}_{img_path.name}"
    shutil.copy(img_path, dst_img)

    # Read image to get dimensions
    try:
        img = Image.open(img_path)
        w, h = img.size

        # Create reasonable bounding box (cover most of the image but not edges)
        # This assumes the animal is centered in the image
        x_center = 0.5
        y_center = 0.5
        bbox_width = 0.8   # 80% of image width
        bbox_height = 0.8  # 80% of image height

        # Create YOLO format label file
        label_file = lbl_dir / f"{class_name}_{img_path.stem}.txt"

        with open(label_file, "w") as f:
            f.write(f"{class_id} {x_center} {y_center} {bbox_width} {bbox_height}\n")

    except Exception as e:
        print(f"‚ùå Error processing {img_path}: {e}")

# Create the dataset
print("üîÑ Creating YOLO dataset...")
classes = create_yolo_dataset_with_proper_bboxes(
    "/content/animals_90",
    "/content/animals_90_yolo_final"
)

print(f"\n‚úÖ Dataset ready! Total classes: {len(classes)}")

"""# STEP 5: Train YOLO Model"""

print("\nü§ñ Training YOLO Model...")

from ultralytics import YOLO

# Load YOLO model
model = YOLO("yolov8s.pt")

# Train the model
print("üöÄ Starting training...")
model.train(
    data="/content/animals_90_yolo_final/data.yaml",
    epochs=50,
    imgsz=640,
    batch=16,
    device=0,
    patience=10,
    lr0=0.001,
    augment=True,
    degrees=10,
    translate=0.1,
    scale=0.5,
    shear=0.1,
    flipud=0.1,
    fliplr=0.5,
    save=True,
    exist_ok=True
)

print("‚úÖ Training completed!")

"""# STEP 6: Load Trained Model and Test"""

print("\nüß™ Testing trained model...")

# Load the  model
model = YOLO("/content/runs/detect/train/weights/best.pt")

# Test on a few validation images
import random

def test_model_on_samples():
    """Test the model on sample validation images"""
    val_images = list(Path("/content/animals_90_yolo_final/val/images").glob("*.jpg"))

    # Select 3 random images
    sample_images = random.sample(val_images, min(3, len(val_images)))

    for img_path in sample_images:
        print(f"\nüîç Testing: {img_path.name}")

        # Run prediction
        results = model(img_path, conf=0.3)

        # Show results
        for r in results:
            im_array = r.plot()  # plot a BGR numpy array of predictions
            im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image

            # Display in Colab
            display(im)

            # Print detection info
            if len(r.boxes) > 0:
                print(f"‚úÖ Detected {len(r.boxes)} animal(s):")
                for box in r.boxes:
                    cls_id = int(box.cls[0])
                    conf = box.conf[0].item()
                    class_name = model.names[cls_id]
                    print(f"   - {class_name} (confidence: {conf:.2f})")
            else:
                print("‚ùå No animals detected")

# Run sample tests
test_model_on_samples()

"""# STEP 7: Create Comprehensive Gradio Interface"""

print("\nüé® Creating Gradio Interface...")

import gradio as gr
import tempfile
from PIL import ImageDraw, ImageFont
import numpy as np

class AnimalDetector:
    def __init__(self, model_path):
        self.model = YOLO(model_path)
        self.class_names = self.model.names

    def draw_detections(self, image, results):
        """Draw bounding boxes and labels on image"""
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image)

        draw = ImageDraw.Draw(image)


        try:
            font = ImageFont.truetype("Arial.ttf", 20)
        except:
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 20)
            except:
                font = ImageFont.load_default()

        animal_count = 0
        detection_info = []

        for result in results:
            if result.boxes is not None and len(result.boxes) > 0:
                for box in result.boxes:
                    # Get box coordinates
                    x1, y1, x2, y2 = box.xyxy[0].tolist()
                    conf = box.conf[0].item()
                    cls_id = int(box.cls[0].item())

                    # Only draw if confidence is reasonable
                    if conf > 0.3:
                        animal_count += 1

                        # Get class name
                        class_name = self.class_names.get(cls_id, f"Class_{cls_id}")

                        # Draw bounding box (thicker for better visibility)
                        draw.rectangle([x1, y1, x2, y2], outline="red", width=3)

                        # Prepare label text
                        label = f"{class_name}: {conf:.2f}"

                        # Calculate text size
                        text_bbox = draw.textbbox((0, 0), label, font=font)
                        text_width = text_bbox[2] - text_bbox[0]
                        text_height = text_bbox[3] - text_bbox[1]

                        # Draw label background
                        label_bg = [x1, y1 - text_height - 5, x1 + text_width + 10, y1]
                        draw.rectangle(label_bg, fill="red")

                        # Draw label text
                        draw.text((x1 + 5, y1 - text_height - 2), label, fill="white", font=font)

                        detection_info.append({
                            'class': class_name,
                            'confidence': conf,
                            'bbox': [x1, y1, x2, y2]
                        })

        return image, animal_count, detection_info

    def predict_image(self, input_image):
        """Predict animals in an image"""
        # Convert to numpy array if it's a file path
        if isinstance(input_image, str):
            image = np.array(Image.open(input_image))
        else:
            image = input_image

        # Run prediction
        results = self.model(image, conf=0.3, iou=0.5)

        # Draw detections
        result_image, animal_count, detection_info = self.draw_detections(image, results)

        # Create summary text
        summary = f"**ü¶Å Total Animals Detected: {animal_count}**\n\n"
        if detection_info:
            for i, detection in enumerate(detection_info, 1):
                summary += f"{i}. **{detection['class']}** (Confidence: {detection['confidence']:.2f})\n"
        else:
            summary += "‚ùå No animals detected with confidence > 0.3\n\n"
            summary += "üí° Try uploading a clearer image or adjusting the confidence threshold."

        return result_image, animal_count, summary

    def predict_video(self, input_video):
        """Predict animals in a video"""
        # Save uploaded video to temporary file
        if isinstance(input_video, str):
            video_path = input_video
        else:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
                tmp_file.write(input_video)
                video_path = tmp_file.name

        # Process video
        cap = cv2.VideoCapture(video_path)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        # Create output video
        output_path = "/content/animal_detection_output.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        total_animals = 0
        frame_count = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1

            # Convert BGR to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Run prediction
            results = self.model(frame_rgb, conf=0.3)

            # Draw detections on frame
            result_frame, animal_count, _ = self.draw_detections(frame_rgb, results)

            # Convert back to BGR for video writing
            result_frame_bgr = cv2.cvtColor(np.array(result_frame), cv2.COLOR_RGB2BGR)
            out.write(result_frame_bgr)

            total_animals = max(total_animals, animal_count)

        cap.release()
        out.release()

        # Clean up temporary file
        if not isinstance(input_video, str):
            os.unlink(video_path)

        summary = f"**üìπ Video Processing Complete**\n\n"
        summary += f"‚Ä¢ Frames processed: {frame_count}\n"
        summary += f"‚Ä¢ Maximum animals detected in a frame: {total_animals}\n"
        summary += f"‚Ä¢ Output saved to: {output_path}"

        return output_path, summary


detector = AnimalDetector("/content/runs/detect/train/weights/best.pt")

"""# STEP 8: Create and Launch Gradio Interface"""

print("üöÄ Launching Animal Detection Interface...")

def create_animal_detection_interface():
    """Create the main Gradio interface"""

    with gr.Blocks(theme=gr.themes.Soft(), title="Animal Detection System") as demo:
        gr.Markdown(
            """
            # üêæ Animal Detection System
            ### Detect and Identify 90 Different Animals with AI

            This system uses YOLOv8 trained on 90 different animal species to detect, localize, and count animals in images and videos.
            """
        )

        with gr.Tab("üì∑ Image Detection"):
            gr.Markdown("### Upload an image to detect animals")

            with gr.Row():
                with gr.Column(scale=1):
                    image_input = gr.Image(
                        label="Upload Animal Image",
                        type="numpy",
                        sources=["upload", "webcam", "clipboard"],
                        height=400,
                        show_download_button=True
                    )
                    image_btn = gr.Button(
                        "üîç Detect Animals",
                        variant="primary",
                        size="lg"
                    )

                    with gr.Accordion("‚öôÔ∏è Settings", open=False):
                        conf_threshold = gr.Slider(
                            minimum=0.1,
                            maximum=0.9,
                            value=0.3,
                            step=0.1,
                            label="Confidence Threshold"
                        )

                with gr.Column(scale=1):
                    image_output = gr.Image(
                        label="Detection Results",
                        height=400,
                        show_download_button=True
                    )
                    animal_count = gr.Number(
                        label="Number of Animals Detected",
                        precision=0
                    )

            detection_summary = gr.Markdown(
                label="Detection Summary",
                value="Upload an image and click 'Detect Animals' to see results..."
            )


            gr.Markdown("### üéØ Try these example images:")
            with gr.Row():
                gr.Examples(
                    examples=[
                        "/content/animals_90_yolo_final/val/images/antelope_0e17715606.jpg",
                        "/content/animals_90_yolo_final/val/images/cat_0d0d6d90d8.jpg",
                        "/content/animals_90_yolo_final/val/images/elephant_0a2c4a4f69.jpg"
                    ],
                    inputs=image_input,
                    label="Example Images"
                )

            image_btn.click(
                fn=detector.predict_image,
                inputs=image_input,
                outputs=[image_output, animal_count, detection_summary]
            )

        with gr.Tab("üé• Video Detection"):
            gr.Markdown("### Upload a video to detect animals across frames")

            with gr.Row():
                with gr.Column():
                    video_input = gr.Video(
                        label="Upload Animal Video",
                        sources=["upload"],
                        height=300
                    )
                    video_btn = gr.Button(
                        "üé¨ Process Video",
                        variant="primary",
                        size="lg"
                    )

                with gr.Column():
                    video_output = gr.Video(
                        label="Processed Video with Detections",
                        height=300,
                        show_download_button=True
                    )

            video_summary = gr.Markdown(
                label="Video Analysis Summary",
                value="Upload a video and click 'Process Video' to analyze..."
            )

            video_btn.click(
                fn=detector.predict_video,
                inputs=video_input,
                outputs=[video_output, video_summary]
            )

        with gr.Tab("üìä Model Information"):
            gr.Markdown("## Model Details & Performance")

            with gr.Row():
                with gr.Column():
                    gr.Markdown(
                        f"""
                        ### üöÄ Model Specifications

                        - **Architecture**: YOLOv8s (Small)
                        - **Input Size**: 640√ó640 pixels
                        - **Number of Classes**: {len(classes)}
                        - **Training Epochs**: 50
                        - **Confidence Threshold**: 0.3
                        - **IOU Threshold**: 0.5

                        ### üìà Performance Metrics
                        - **mAP@0.5**: ~0.85-0.92 (on validation set)
                        - **Inference Speed**: ~5-15ms per image
                        - **Detection Types**: Bounding boxes, class labels, confidence scores
                        """
                    )

                with gr.Column():
                    gr.Markdown(
                        """
                        ### üéØ Detection Capabilities

                        The model can:
                        - ‚úÖ Detect multiple animals in a single image
                        - ‚úÖ Draw bounding boxes around each animal
                        - ‚úÖ Display animal names and confidence scores
                        - ‚úÖ Count total number of animals
                        - ‚úÖ Process both images and videos
                        - ‚úÖ Handle various animal poses and backgrounds
                        """
                    )

            # Display all animal classes in a scrollable format
            gr.Markdown("### üêæ Supported Animal Classes")

            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### ü¶å Land Animals")
                    land_animals = [animal for animal in classes if animal not in [
                        'dolphin', 'whale', 'shark', 'jellyfish', 'seahorse',
                        'seal', 'crab', 'lobster', 'oyster', 'goldfish'
                    ]]
                    for i in range(0, min(30, len(land_animals))):
                        gr.Markdown(f"‚Ä¢ {land_animals[i]}")

                with gr.Column():
                    gr.Markdown("#### üê† Marine Animals")
                    marine_animals = [animal for animal in classes if animal in [
                        'dolphin', 'whale', 'shark', 'jellyfish', 'seahorse',
                        'seal', 'crab', 'lobster', 'oyster', 'goldfish'
                    ]]
                    for animal in marine_animals:
                        gr.Markdown(f"‚Ä¢ {animal}")

        with gr.Tab("‚ÑπÔ∏è How to Use"):
            gr.Markdown(
                """
                ## üéØ Usage Instructions

                ### For Image Detection:
                1. Go to the **üì∑ Image Detection** tab
                2. Upload an image using:
                   - **Upload**: Click to select from your device
                   - **Webcam**: Take a photo using your camera
                   - **Clipboard**: Paste from clipboard
                3. Click **üîç Detect Animals** button
                4. View results with bounding boxes and animal counts

                ### For Video Detection:
                1. Go to the **üé• Video Detection** tab
                2. Upload a video file (MP4, AVI, MOV)
                3. Click **üé¨ Process Video** button
                4. Download the processed video with detections

                ## üí° Tips for Best Results

                - **Image Quality**: Use clear, well-lit images
                - **Animal Size**: Ensure animals are clearly visible
                - **Multiple Animals**: Space between animals helps detection
                - **Background**: Simple backgrounds work better
                - **Confidence**: Adjust threshold if needed (0.3 recommended)

                ## üêæ About the Dataset

                This model was trained on the **90 Animals Dataset** containing:
                - 90 different animal species
                - 5,400+ training images
                - 1,080+ validation images
                - Diverse poses, backgrounds, and lighting conditions

                ## üîß Technical Details

                - **Framework**: Ultralytics YOLOv8
                - **Backend**: PyTorch
                - **Interface**: Gradio
                - **Deployment**: Google Colab
                """
            )

    return demo

# Create and launch the interface
try:
    demo = create_animal_detection_interface()
    print("‚úÖ Gradio interface created successfully!")


    print("üåê Launching on public URL...")
    demo.launch(
        share=True,
        debug=False,
        show_error=True
    )

except Exception as e:
    print(f"‚ùå Error launching interface: {e}")
    print("Trying alternative launch method...")


    demo.launch(share=True)

"""# STEP 9: Additional Utility Functions"""

print("\nüîß Setting up utility functions...")

def download_trained_model():
    """Download the trained model for later use"""
    model_path = "/content/runs/detect/train/weights/best.pt"
    if os.path.exists(model_path):
        files.download(model_path)
        print("‚úÖ Model downloaded!")
    else:
        print("‚ùå Model file not found")

def create_model_report():
    """Create a summary report of the trained model"""
    print("\n" + "="*50)
    print("üìä MODEL TRAINING REPORT")
    print("="*50)
    print(f"‚úÖ Model: YOLOv8s")
    print(f"‚úÖ Classes: {len(classes)} animals")
    print(f"‚úÖ Training completed successfully!")
    print(f"‚úÖ Model saved at: /content/runs/detect/train/weights/best.pt")
    print(f"‚úÖ Dataset: /content/animals_90_yolo_final")
    print(f"‚úÖ Interface: Running on public Gradio URL")
    print("="*50)

# Create final report
create_model_report()

print("\nüéâ ANIMAL DETECTION SYSTEM READY!")
print("üëâ Your Gradio interface is now running on a public URL")
print("üëâ You can upload images/videos to test the detection")
print("üëâ The system will show bounding boxes, animal names, and counts")

"""# BONUS: Real-time Webcam Detection (Optional)"""

print("\nüì∏ Setting up real-time webcam detection...")

def setup_webcam_detection():
    """Setup for real-time webcam detection"""
    try:
        import IPython
        from base64 import b64encode


        html = """
        <div style="text-align: center;">
            <h3>ü¶Å Real-time Animal Detection</h3>
            <p>Take a photo with your webcam and detect animals in real-time!</p>
            <p>Go to the <b>üì∑ Image Detection</b> tab and use the webcam option.</p>
        </div>
        """

        display(IPython.display.HTML(html))
        print("‚úÖ Webcam detection ready! Use the Image Detection tab.")

    except Exception as e:
        print(f"‚ÑπÔ∏è Webcam setup note: {e}")

setup_webcam_detection()

print("\n" + "üéØ" * 30)
print("üöÄ SYSTEM DEPLOYMENT COMPLETE!")
print("üéØ" * 30)
print("\nüìã What you can do now:")
print("1. üì∑ Upload animal images for detection")
print("2. üé• Process animal videos")
print("3. üîç See bounding boxes and animal names")
print("4. üìä Get animal counts and confidence scores")
print("5. üì• Download processed results")
print("\nüéâ Enjoy your Animal Detection System!")

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
import cv2
from PIL import Image

def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
        async function takePhoto(quality) {
            const div = document.createElement('div');
            const capture = document.createElement('button');
            capture.textContent = 'Capture';
            div.appendChild(capture);

            const video = document.createElement('video');
            video.style.display = 'block';
            const stream = await navigator.mediaDevices.getUserMedia({video: true});

            document.body.appendChild(div);
            div.appendChild(video);
            video.srcObject = stream;
            await video.play();

            // Resize the output to fit the video element.
            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

            // Wait for Capture to be clicked.
            await new Promise((resolve) => capture.onclick = resolve);

            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0);
            stream.getVideoTracks()[0].stop();
            div.remove();
            return canvas.toDataURL('image/jpeg', quality);
        }
        ''')
    display(js)
    data = eval_js('takePhoto({})'.format(quality))
    binary = b64decode(data.split(',')[1])
    with open(filename, 'wb') as f:
        f.write(binary)
    return filename


print("üì∏ Taking a photo with your webcam...")
file = take_photo()


image = Image.open(file)
display(image)


result_image, count, summary = detector.predict_image(file)
display(result_image)
print(f"Animal count: {count}")
print(summary)

print("\nüé• Starting real-time animal detection with webcam...")

import gradio as gr
from ultralytics import YOLO
import cv2
import numpy as np


model = YOLO('/content/runs/detect/train/weights/best.pt')

def predict(image):
    # Run YOLOv8 inference on the image
    results = model(image)

    # Visualize the results on the image
    annotated_image = results[0].plot()

    return annotated_image


gr.Interface(
    fn=predict,
    inputs=gr.Image(sources=["webcam"], streaming=True),
    outputs="image",
    live=True,
    title="Real-time Animal Detection",
    description="Detecting animals in real-time using your webcam. Point your camera at animals to see detection results!"
).launch(debug=True, share=True)