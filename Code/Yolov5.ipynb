{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcYzFkKm4ngT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tv-ctrV4pYd"
      },
      "source": [
        "# SECTION 1: Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKOJQF-E40F5"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q opencv-python-headless\n",
        "!pip install -q pillow\n",
        "!pip install -q matplotlib\n",
        "!pip install -q seaborn\n",
        "!pip install -q Flask\n",
        "!pip install -q fastapi uvicorn\n",
        "!pip install -q gradio  # For HuggingFace deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOs-JoHH44iS",
        "outputId": "c6144784-24cb-46fa-abc6-f1a29df2627e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17739, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 17739 (delta 57), reused 31 (delta 31), pack-reused 17643 (from 4)\u001b[K\n",
            "Receiving objects: 100% (17739/17739), 17.11 MiB | 24.00 MiB/s, done.\n",
            "Resolving deltas: 100% (12044/12044), done.\n",
            "/content/yolov5\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Clone YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!pip install -q -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAe-3edE5SzI"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-gy22x25NPb",
        "outputId": "09e2980f-9a55-45e3-b509-0c47c76479cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì All packages installed successfully!\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "from IPython.display import display, HTML\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"‚úì All packages installed successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZteDJAWs5hiD"
      },
      "source": [
        "# SECTION 2: Download Pre-trained YOLOv5 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz8z0eXl5Vt2",
        "outputId": "ce2204b3-9fcb-4941-8e0f-2497bbfa36f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading yolov5x model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 üöÄ 2025-11-18 Python-3.12.12 torch-2.8.0+cu126 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt to yolov5x.pt...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 166M/166M [00:00<00:00, 259MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients, 205.5 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Using YOLOv5x (most accurate) for best results\n",
        "# You can also use yolov5s (fastest), yolov5m, yolov5l based on speed needs\n",
        "MODEL_TYPE = 'yolov5x'  # Change to 'yolov5s' for faster inference\n",
        "\n",
        "print(f\"Loading {MODEL_TYPE} model...\")\n",
        "model = torch.hub.load('ultralytics/yolov5', MODEL_TYPE, pretrained=True)\n",
        "\n",
        "# Configure model for better detection\n",
        "model.conf = 0.4  # Confidence threshold\n",
        "model.iou = 0.45  # NMS IOU threshold\n",
        "model.classes = None  # Detect all classes, we'll filter for animals\n",
        "\n",
        "print(\"‚úì Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOCxhoRC50IK"
      },
      "source": [
        "# SECTION 3: Animal Class Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBlLT6h75mHq"
      },
      "outputs": [],
      "source": [
        " # COCO dataset animal classes (YOLOv5 is trained on COCO)\n",
        "ANIMAL_CLASSES = {\n",
        "    14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep',\n",
        "    19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe'\n",
        "}\n",
        "\n",
        "# Extended animal detection including wildlife\n",
        "WILDLIFE_CLASSES = list(ANIMAL_CLASSES.keys())\n",
        "\n",
        "def filter_animal_detections(results):\n",
        "    \"\"\"Filter detections to include only animals\"\"\"\n",
        "    df = results.pandas().xyxy[0]\n",
        "    animal_df = df[df['class'].isin(WILDLIFE_CLASSES)]\n",
        "    return animal_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_raWCphN588E"
      },
      "source": [
        "# SECTION 4: Enhanced Detection with Color & Shape Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw24UpyF54xo"
      },
      "outputs": [],
      "source": [
        "class AnimalDetector:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.detection_history = []\n",
        "\n",
        "    def detect_animals(self, image_path, save_path='output'):\n",
        "        \"\"\"Main detection function with enhanced features\"\"\"\n",
        "        # Read image\n",
        "        img = cv2.imread(str(image_path))\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Run YOLOv5 detection\n",
        "        results = self.model(img_rgb)\n",
        "\n",
        "        # Filter for animals only\n",
        "        animal_df = filter_animal_detections(results)\n",
        "\n",
        "        # Enhanced visualization with color analysis\n",
        "        annotated_img = self.annotate_image(img_rgb, animal_df)\n",
        "\n",
        "        # Color and shape analysis for each detection\n",
        "        enhanced_results = self.analyze_detections(img_rgb, animal_df)\n",
        "\n",
        "        # Save results\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        output_path = os.path.join(save_path, f\"detected_{Path(image_path).name}\")\n",
        "        cv2.imwrite(output_path, cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Store detection history\n",
        "        self.detection_history.append({\n",
        "            'image': image_path,\n",
        "            'detections': len(animal_df),\n",
        "            'animals': enhanced_results\n",
        "        })\n",
        "\n",
        "        return annotated_img, enhanced_results, output_path\n",
        "\n",
        "    def annotate_image(self, img, detections):\n",
        "        \"\"\"Draw bounding boxes with enhanced styling\"\"\"\n",
        "        img_copy = img.copy()\n",
        "\n",
        "        # Color palette for different animals\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
        "\n",
        "        for idx, row in detections.iterrows():\n",
        "            x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
        "            conf = row['confidence']\n",
        "            cls = int(row['class'])\n",
        "            label = ANIMAL_CLASSES.get(cls, 'animal')\n",
        "\n",
        "            # Get color for this class\n",
        "            color = tuple(int(c * 255) for c in colors[cls % 10][:3])\n",
        "\n",
        "            # Draw thick rectangle\n",
        "            cv2.rectangle(img_copy, (x1, y1), (x2, y2), color, 3)\n",
        "\n",
        "            # Add label with background\n",
        "            label_text = f\"{label} {conf:.2f}\"\n",
        "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "            font_scale = 0.7\n",
        "            thickness = 2\n",
        "\n",
        "            (text_w, text_h), _ = cv2.getTextSize(label_text, font, font_scale, thickness)\n",
        "            cv2.rectangle(img_copy, (x1, y1-text_h-10), (x1+text_w+10, y1), color, -1)\n",
        "            cv2.putText(img_copy, label_text, (x1+5, y1-5), font, font_scale, (255,255,255), thickness)\n",
        "\n",
        "            # Add corner markers for professional look\n",
        "            corner_len = 20\n",
        "            cv2.line(img_copy, (x1, y1), (x1+corner_len, y1), color, 4)\n",
        "            cv2.line(img_copy, (x1, y1), (x1, y1+corner_len), color, 4)\n",
        "            cv2.line(img_copy, (x2, y1), (x2-corner_len, y1), color, 4)\n",
        "            cv2.line(img_copy, (x2, y1), (x2, y1+corner_len), color, 4)\n",
        "            cv2.line(img_copy, (x1, y2), (x1+corner_len, y2), color, 4)\n",
        "            cv2.line(img_copy, (x1, y2), (x1, y2-corner_len), color, 4)\n",
        "            cv2.line(img_copy, (x2, y2), (x2-corner_len, y2), color, 4)\n",
        "            cv2.line(img_copy, (x2, y2), (x2, y2-corner_len), color, 4)\n",
        "\n",
        "        return img_copy\n",
        "\n",
        "    def analyze_detections(self, img, detections):\n",
        "        \"\"\"Enhanced analysis: color distribution and shape metrics\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for idx, row in detections.iterrows():\n",
        "            x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
        "            cls = int(row['class'])\n",
        "            conf = row['confidence']\n",
        "\n",
        "            # Extract ROI\n",
        "            roi = img[y1:y2, x1:x2]\n",
        "\n",
        "            # Color analysis\n",
        "            color_info = self.analyze_color(roi)\n",
        "\n",
        "            # Shape metrics\n",
        "            shape_info = self.analyze_shape(x1, y1, x2, y2)\n",
        "\n",
        "            results.append({\n",
        "                'animal': ANIMAL_CLASSES.get(cls, 'animal'),\n",
        "                'confidence': float(conf),\n",
        "                'bbox': [x1, y1, x2, y2],\n",
        "                'color_analysis': color_info,\n",
        "                'shape_metrics': shape_info\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def analyze_color(self, roi):\n",
        "        \"\"\"Analyze dominant colors in detected region\"\"\"\n",
        "        if roi.size == 0:\n",
        "            return {'dominant_color': 'N/A', 'brightness': 0}\n",
        "\n",
        "        # Calculate average color\n",
        "        avg_color = roi.mean(axis=(0,1))\n",
        "\n",
        "        # Determine dominant color name\n",
        "        r, g, b = avg_color\n",
        "        dominant = 'red' if r > g and r > b else 'green' if g > r and g > b else 'blue'\n",
        "\n",
        "        # Calculate brightness\n",
        "        brightness = np.mean(roi)\n",
        "\n",
        "        return {\n",
        "            'dominant_color': dominant,\n",
        "            'rgb_values': [int(r), int(g), int(b)],\n",
        "            'brightness': float(brightness)\n",
        "        }\n",
        "\n",
        "    def analyze_shape(self, x1, y1, x2, y2):\n",
        "        \"\"\"Calculate shape metrics\"\"\"\n",
        "        width = x2 - x1\n",
        "        height = y2 - y1\n",
        "        area = width * height\n",
        "        aspect_ratio = width / height if height > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'width': width,\n",
        "            'height': height,\n",
        "            'area': area,\n",
        "            'aspect_ratio': round(aspect_ratio, 2)\n",
        "        }\n",
        "\n",
        "    def create_visualization_dashboard(self, results, save_path='output'):\n",
        "        \"\"\"Create comprehensive visualization dashboard\"\"\"\n",
        "        if not results:\n",
        "            print(\"No animals detected!\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle('Animal Detection Analysis Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # 1. Detection counts\n",
        "        animal_counts = {}\n",
        "        for r in results:\n",
        "            animal = r['animal']\n",
        "            animal_counts[animal] = animal_counts.get(animal, 0) + 1\n",
        "\n",
        "        axes[0, 0].bar(animal_counts.keys(), animal_counts.values(), color='skyblue')\n",
        "        axes[0, 0].set_title('Animal Detection Counts')\n",
        "        axes[0, 0].set_xlabel('Animal Type')\n",
        "        axes[0, 0].set_ylabel('Count')\n",
        "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # 2. Confidence scores\n",
        "        confidences = [r['confidence'] for r in results]\n",
        "        animals = [r['animal'] for r in results]\n",
        "        axes[0, 1].barh(animals, confidences, color='lightgreen')\n",
        "        axes[0, 1].set_title('Detection Confidence Scores')\n",
        "        axes[0, 1].set_xlabel('Confidence')\n",
        "        axes[0, 1].set_xlim([0, 1])\n",
        "\n",
        "        # 3. Size distribution\n",
        "        areas = [r['shape_metrics']['area'] for r in results]\n",
        "        axes[1, 0].hist(areas, bins=10, color='coral', edgecolor='black')\n",
        "        axes[1, 0].set_title('Animal Size Distribution')\n",
        "        axes[1, 0].set_xlabel('Bounding Box Area (pixels¬≤)')\n",
        "        axes[1, 0].set_ylabel('Frequency')\n",
        "\n",
        "        # 4. Aspect ratios\n",
        "        aspect_ratios = [r['shape_metrics']['aspect_ratio'] for r in results]\n",
        "        axes[1, 1].scatter(range(len(aspect_ratios)), aspect_ratios,\n",
        "                          c=confidences, cmap='viridis', s=100, alpha=0.7)\n",
        "        axes[1, 1].set_title('Aspect Ratio Analysis')\n",
        "        axes[1, 1].set_xlabel('Detection Index')\n",
        "        axes[1, 1].set_ylabel('Aspect Ratio (W/H)')\n",
        "        axes[1, 1].axhline(y=1, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        dashboard_path = os.path.join(save_path, 'analysis_dashboard.png')\n",
        "        plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"‚úì Dashboard saved to: {dashboard_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWCRIZXD6gOB"
      },
      "source": [
        "# SECTION 5: Download Sample Dataset (Wildlife Images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGck32pM6W0U",
        "outputId": "5b882fa2-6b49-4d41-9903-4ed307182d51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading sample wildlife images...\n",
            "For best results, upload your own animal images to 'sample_images/' folder\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Download sample images for testing\n",
        "!mkdir -p sample_images\n",
        "\n",
        "# Using wget to download sample wildlife images\n",
        "print(\"Downloading sample wildlife images...\")\n",
        "\n",
        "# Sample URLs (you can replace with your own dataset)\n",
        "sample_urls = [\n",
        "    \"https://images.unsplash.com/photo-1564349683136-77e08dba1ef7\",  # Tiger\n",
        "    \"https://images.unsplash.com/photo-1535591273668-578e31182c4f\",  # Elephant\n",
        "    \"https://images.unsplash.com/photo-1549366021-9f761d450615\",  # Dog\n",
        "]\n",
        "\n",
        "# Note: In practice, you should use a proper dataset like:\n",
        "# - COCO animals subset\n",
        "# - Open Images Dataset (animals)\n",
        "# - Animals-10 dataset from Kaggle\n",
        "\n",
        "print(\"For best results, upload your own animal images to 'sample_images/' folder\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR0sYlKw6t75"
      },
      "source": [
        "# SECTION 6: Run Detection on Sample Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaC6N0w76l_U",
        "outputId": "c77518a0-1a12-40ee-e556-2994025b5be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No images found in sample_images folder!\n"
          ]
        }
      ],
      "source": [
        "detector = AnimalDetector(model)\n",
        "\n",
        "# Process all images in sample_images folder\n",
        "image_folder = 'sample_images'\n",
        "output_folder = 'detection_results'\n",
        "\n",
        "if os.path.exists(image_folder):\n",
        "    image_files = [f for f in os.listdir(image_folder)\n",
        "                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    if image_files:\n",
        "        print(f\"\\nProcessing {len(image_files)} images...\\n\")\n",
        "        all_results = []\n",
        "\n",
        "        for img_file in image_files:\n",
        "            img_path = os.path.join(image_folder, img_file)\n",
        "            print(f\"Processing: {img_file}\")\n",
        "\n",
        "            annotated, results, output_path = detector.detect_animals(img_path, output_folder)\n",
        "            all_results.extend(results)\n",
        "\n",
        "            # Display result\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plt.imshow(annotated)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Detection Results: {img_file}')\n",
        "            plt.show()\n",
        "\n",
        "            # Print detailed results\n",
        "            print(f\"  ‚Üí Found {len(results)} animal(s)\")\n",
        "            for i, r in enumerate(results, 1):\n",
        "                print(f\"    {i}. {r['animal'].upper()}: {r['confidence']:.2%} confidence\")\n",
        "                print(f\"       Color: {r['color_analysis']['dominant_color']}\")\n",
        "                print(f\"       Size: {r['shape_metrics']['width']}x{r['shape_metrics']['height']}px\")\n",
        "            print()\n",
        "\n",
        "        # Create overall dashboard\n",
        "        if all_results:\n",
        "            detector.create_visualization_dashboard(all_results, output_folder)\n",
        "\n",
        "            # Save results to JSON\n",
        "            json_path = os.path.join(output_folder, 'detection_results.json')\n",
        "            with open(json_path, 'w') as f:\n",
        "                json.dump(all_results, f, indent=2)\n",
        "            print(f\"‚úì Results saved to: {json_path}\")\n",
        "    else:\n",
        "        print(\"No images found in sample_images folder!\")\n",
        "else:\n",
        "    print(\"Please create 'sample_images' folder and add images!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFqzWMEz62lm",
        "outputId": "68fbaedb-875a-4b5a-cc09-a9f903c7d241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading sample images...\n",
            "‚úì Downloaded dog.jpg\n",
            "‚úì Downloaded cat.jpg\n",
            "‚úì Downloaded elephant.jpg\n",
            "‚úì Downloaded horse.jpg\n",
            "‚úì Downloaded bird.jpg\n",
            "\n",
            "‚úì Sample images ready!\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "# Create sample_images folder\n",
        "os.makedirs('sample_images', exist_ok=True)\n",
        "\n",
        "# Sample animal images (free to use)\n",
        "sample_images = {\n",
        "    'dog.jpg': 'https://images.unsplash.com/photo-1583511655857-d19b40a7a54e?w=800',\n",
        "    'cat.jpg': 'https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=800',\n",
        "    'elephant.jpg': 'https://images.unsplash.com/photo-1564760055775-d63b17a55c44?w=800',\n",
        "    'horse.jpg': 'https://images.unsplash.com/photo-1553284965-83fd3e82fa5a?w=800',\n",
        "    'bird.jpg': 'https://images.unsplash.com/photo-1444464666168-49d633b86797?w=800'\n",
        "}\n",
        "\n",
        "print(\"Downloading sample images...\")\n",
        "for filename, url in sample_images.items():\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, f'sample_images/{filename}')\n",
        "        print(f\"‚úì Downloaded {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Failed to download {filename}: {e}\")\n",
        "\n",
        "print(\"\\n‚úì Sample images ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q4hjbL49SfV",
        "outputId": "d2a474ee-d1cb-4041-b03e-ebf294dc245e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing 5 images...\n",
            "\n",
            "Processing: cat.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚Üí Found 1 animal(s)\n",
            "    1. CAT: 75.56% confidence\n",
            "       Color: green\n",
            "       Size: 465x484px\n",
            "\n",
            "Processing: dog.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚Üí Found 1 animal(s)\n",
            "    1. DOG: 94.49% confidence\n",
            "       Color: green\n",
            "       Size: 204x349px\n",
            "\n",
            "Processing: horse.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚Üí Found 1 animal(s)\n",
            "    1. HORSE: 95.93% confidence\n",
            "       Color: blue\n",
            "       Size: 464x332px\n",
            "\n",
            "Processing: elephant.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚Üí Found 2 animal(s)\n",
            "    1. ELEPHANT: 95.80% confidence\n",
            "       Color: red\n",
            "       Size: 164x201px\n",
            "    2. ELEPHANT: 94.94% confidence\n",
            "       Color: red\n",
            "       Size: 232x163px\n",
            "\n",
            "Processing: bird.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚Üí Found 1 animal(s)\n",
            "    1. BIRD: 94.33% confidence\n",
            "       Color: green\n",
            "       Size: 291x342px\n",
            "\n",
            "‚úì Dashboard saved to: detection_results/analysis_dashboard.png\n",
            "‚úì Results saved to: detection_results/detection_results.json\n"
          ]
        }
      ],
      "source": [
        "detector = AnimalDetector(model)\n",
        "\n",
        "# Process all images in sample_images folder\n",
        "image_folder = 'sample_images'\n",
        "output_folder = 'detection_results'\n",
        "\n",
        "if os.path.exists(image_folder):\n",
        "    image_files = [f for f in os.listdir(image_folder)\n",
        "                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    if image_files:\n",
        "        print(f\"\\nProcessing {len(image_files)} images...\\n\")\n",
        "        all_results = []\n",
        "\n",
        "        for img_file in image_files:\n",
        "            img_path = os.path.join(image_folder, img_file)\n",
        "            print(f\"Processing: {img_file}\")\n",
        "\n",
        "            annotated, results, output_path = detector.detect_animals(img_path, output_folder)\n",
        "            all_results.extend(results)\n",
        "\n",
        "            # Display result\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plt.imshow(annotated)\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Detection Results: {img_file}')\n",
        "            plt.show()\n",
        "\n",
        "            # Print detailed results\n",
        "            print(f\"  ‚Üí Found {len(results)} animal(s)\")\n",
        "            for i, r in enumerate(results, 1):\n",
        "                print(f\"    {i}. {r['animal'].upper()}: {r['confidence']:.2%} confidence\")\n",
        "                print(f\"       Color: {r['color_analysis']['dominant_color']}\")\n",
        "                print(f\"       Size: {r['shape_metrics']['width']}x{r['shape_metrics']['height']}px\")\n",
        "            print()\n",
        "\n",
        "        # Create overall dashboard\n",
        "        if all_results:\n",
        "            detector.create_visualization_dashboard(all_results, output_folder)\n",
        "\n",
        "            # Save results to JSON\n",
        "            json_path = os.path.join(output_folder, 'detection_results.json')\n",
        "            with open(json_path, 'w') as f:\n",
        "                json.dump(all_results, f, indent=2)\n",
        "            print(f\"‚úì Results saved to: {json_path}\")\n",
        "    else:\n",
        "        print(\"No images found in sample_images folder!\")\n",
        "else:\n",
        "    print(\"Please create 'sample_images' folder and add images!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtOD2eGk98pB"
      },
      "source": [
        "SECTION 7: Real-time Webcam Detection (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoFqK7OA9Vef"
      },
      "outputs": [],
      "source": [
        "def detect_from_webcam(duration=30):\n",
        "    \"\"\"Run detection on webcam feed\"\"\"\n",
        "    from google.colab.patches import cv2_imshow\n",
        "\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    print(f\"Starting webcam detection for {duration} seconds...\")\n",
        "    start_time = cv2.getTickCount()\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert BGR to RGB\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Detect\n",
        "        results = model(frame_rgb)\n",
        "        animal_df = filter_animal_detections(results)\n",
        "\n",
        "        # Annotate\n",
        "        annotated = detector.annotate_image(frame_rgb, animal_df)\n",
        "\n",
        "        # Show\n",
        "        cv2_imshow(cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Check duration\n",
        "        elapsed = (cv2.getTickCount() - start_time) / cv2.getTickFrequency()\n",
        "        if elapsed > duration:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    print(\"Webcam detection completed!\")\n",
        "\n",
        "# Uncomment to run webcam detection\n",
        "# detect_from_webcam(duration=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8XabWAF-QJ9"
      },
      "source": [
        "# SECTION 8: Export Model for Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_OP3Uqv-DOi",
        "outputId": "1f1b74f9-5cc5-4532-9199-b350b1dffd14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "DEPLOYMENT PREPARATION\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3006701166.py:12: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(model.model, dummy_input,\n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:695: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:101: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONNX export skipped: Module onnx is not installed!\n",
            "‚úì Detection history saved to: deployment_ready/detection_history.json\n",
            "\n",
            "============================================================\n",
            "PROJECT COMPLETE! üéâ\n",
            "============================================================\n",
            "\n",
            "Next Steps for Deployment:\n",
            "1. Flask API: Use the saved model and create endpoints\n",
            "2. FastAPI: Higher performance, async support\n",
            "3. HuggingFace Spaces: Use Gradio interface (see below)\n",
            "\n",
            "All outputs saved in 'detection_results/' and 'deployment_ready/'\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DEPLOYMENT PREPARATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save model for deployment\n",
        "deployment_folder = 'deployment_ready'\n",
        "os.makedirs(deployment_folder, exist_ok=True)\n",
        "\n",
        "# Export to ONNX for production\n",
        "try:\n",
        "    dummy_input = torch.randn(1, 3, 640, 640)\n",
        "    torch.onnx.export(model.model, dummy_input,\n",
        "                     f\"{deployment_folder}/animal_detector.onnx\",\n",
        "                     export_params=True,\n",
        "                     opset_version=11,\n",
        "                     input_names=['input'],\n",
        "                     output_names=['output'])\n",
        "    print(\"‚úì Model exported to ONNX format\")\n",
        "except Exception as e:\n",
        "    print(f\"ONNX export skipped: {e}\")\n",
        "\n",
        "# Save detection history\n",
        "history_path = os.path.join(deployment_folder, 'detection_history.json')\n",
        "with open(history_path, 'w') as f:\n",
        "    json.dump(detector.detection_history, f, indent=2)\n",
        "print(f\"‚úì Detection history saved to: {history_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROJECT COMPLETE! üéâ\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nNext Steps for Deployment:\")\n",
        "print(\"1. Flask API: Use the saved model and create endpoints\")\n",
        "print(\"2. FastAPI: Higher performance, async support\")\n",
        "print(\"3. HuggingFace Spaces: Use Gradio interface (see below)\")\n",
        "print(\"\\nAll outputs saved in 'detection_results/' and 'deployment_ready/'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee5FbWjx-gQR"
      },
      "source": [
        "# SECTION 9: Gradio Interface for HuggingFace Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "rDjuO3sX-Vhr",
        "outputId": "848b35c9-a496-4f33-aa76-7e465dcae969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Launching Gradio interface...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f7def4a7bc38851744.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://f7def4a7bc38851744.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_detect(image):\n",
        "    \"\"\"Gradio inference function\"\"\"\n",
        "    if image is None:\n",
        "        return None, \"Please upload an image\"\n",
        "\n",
        "    # Convert to RGB if needed\n",
        "    if len(image.shape) == 2:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # Run detection\n",
        "    results = model(image)\n",
        "    animal_df = filter_animal_detections(results)\n",
        "\n",
        "    # Annotate\n",
        "    annotated = detector.annotate_image(image, animal_df)\n",
        "\n",
        "    # Create results text\n",
        "    results_text = f\"üéØ Detected {len(animal_df)} animal(s):\\n\\n\"\n",
        "    for idx, row in animal_df.iterrows():\n",
        "        animal = ANIMAL_CLASSES.get(int(row['class']), 'animal')\n",
        "        conf = row['confidence']\n",
        "        results_text += f\"‚Ä¢ {animal.upper()}: {conf:.1%} confidence\\n\"\n",
        "\n",
        "    return annotated, results_text\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_detect,\n",
        "    inputs=gr.Image(label=\"Upload Animal Image\"),\n",
        "    outputs=[\n",
        "        gr.Image(label=\"Detection Results\"),\n",
        "        gr.Textbox(label=\"Detected Animals\", lines=5)\n",
        "    ],\n",
        "    title=\"üêæ AVISHKAR 1.0 - Animal Detection System\",\n",
        "    description=\"Upload an image to detect and analyze animals using YOLOv5\",\n",
        "    examples=[],\n",
        "    theme=gr.themes.Soft()\n",
        ")\n",
        "\n",
        "# Launch interface\n",
        "print(\"\\nLaunching Gradio interface...\")\n",
        "demo.launch(share=True, debug=True)\n",
        "\n",
        "print(\"\\n‚ú® Copy the public URL above to share your demo!\")\n",
        "print(\"üì§ Deploy to HuggingFace Spaces: https://huggingface.co/spaces\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3bIPQll-oNK"
      },
      "outputs": [],
      "source": [
        "# CHECK MODEL LOCATION AND SAVE IT PROPERLY\n",
        "print(\"üîç Checking Model Location...\")\n",
        "\n",
        "# Your current model is loaded from torch.hub and exists only in memory\n",
        "print(f\"Model type: {type(model)}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# The model was downloaded to torch hub cache, but let's save it locally\n",
        "MODEL_SAVE_PATH = '/content/animal_detection_model.pt'\n",
        "\n",
        "print(\"\\nüíæ Saving model to local file...\")\n",
        "try:\n",
        "    # Save the entire model\n",
        "    torch.save(model, MODEL_SAVE_PATH)\n",
        "    print(f\"‚úÖ Model saved to: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # Also save just the state dict (lighter)\n",
        "    STATE_DICT_PATH = '/content/animal_detection_state_dict.pt'\n",
        "    torch.save(model.state_dict(), STATE_DICT_PATH)\n",
        "    print(f\"‚úÖ State dict saved to: {STATE_DICT_PATH}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error saving model: {e}\")\n",
        "\n",
        "# Check if files were created\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    file_size = os.path.getsize(MODEL_SAVE_PATH) / (1024 * 1024)  # MB\n",
        "    print(f\"üì¶ Model file size: {file_size:.2f} MB\")\n",
        "else:\n",
        "    print(\"‚ùå Model file was not created\")\n",
        "\n",
        "# Let's also check the torch hub cache location\n",
        "import torch.hub as hub\n",
        "print(f\"\\nüè† Torch hub cache directory: {hub.get_dir()}\")\n",
        "\n",
        "# List files in the hub directory to see cached models\n",
        "hub_dir = hub.get_dir()\n",
        "if os.path.exists(hub_dir):\n",
        "    print(f\"\\nüìÅ Contents of hub directory:\")\n",
        "    for item in os.listdir(hub_dir):\n",
        "        item_path = os.path.join(hub_dir, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"  üìÇ {item}\")\n",
        "            # Check ultralytics_yolov5_master specifically\n",
        "            if 'ultralytics' in item.lower():\n",
        "                yolov5_path = os.path.join(item_path, 'yolov5x.pt')\n",
        "                if os.path.exists(yolov5_path):\n",
        "                    print(f\"    üéØ YOLOv5 model found: {yolov5_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAXqzB74YTMs"
      },
      "outputs": [],
      "source": [
        "# PROPER WAY TO SAVE AND LOAD YOLOv5 MODELS\n",
        "print(\"\\nüöÄ PROPER MODEL MANAGEMENT FOR YOLOv5\")\n",
        "\n",
        "def save_yolov5_model_properly():\n",
        "    \"\"\"Proper way to save YOLOv5 model for deployment\"\"\"\n",
        "\n",
        "    # Method 1: Save using torch.save (entire model)\n",
        "    model_path_1 = '/content/yolov5x_complete.pt'\n",
        "    torch.save(model, model_path_1)\n",
        "    print(f\"‚úÖ Complete model saved: {model_path_1}\")\n",
        "\n",
        "    # Method 2: Save state dict only\n",
        "    model_path_2 = '/content/yolov5x_state_dict.pt'\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'model_config': {\n",
        "            'type': 'yolov5x',\n",
        "            'classes': ANIMAL_CLASSES,\n",
        "            'confidence': 0.4,\n",
        "            'iou': 0.45\n",
        "        }\n",
        "    }, model_path_2)\n",
        "    print(f\"‚úÖ State dict + config saved: {model_path_2}\")\n",
        "\n",
        "    # Method 3: Export to ONNX for production\n",
        "    try:\n",
        "        # Create dummy input\n",
        "        dummy_input = torch.randn(1, 3, 640, 640, device=next(model.parameters()).device)\n",
        "\n",
        "        # Export to ONNX\n",
        "        onnx_path = '/content/yolov5x_animal_detector.onnx'\n",
        "        torch.onnx.export(\n",
        "            model,\n",
        "            dummy_input,\n",
        "            onnx_path,\n",
        "            input_names=['input'],\n",
        "            output_names=['output'],\n",
        "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
        "            opset_version=12\n",
        "        )\n",
        "        print(f\"‚úÖ ONNX model exported: {onnx_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è ONNX export failed: {e}\")\n",
        "\n",
        "    return model_path_1, model_path_2\n",
        "\n",
        "# Save models properly\n",
        "saved_paths = save_yolov5_model_properly()\n",
        "\n",
        "# Verify the saved models can be loaded\n",
        "print(\"\\nüîç Verifying saved models...\")\n",
        "for path in saved_paths:\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            # Load the model\n",
        "            loaded_model = torch.load(path)\n",
        "            print(f\"‚úÖ Successfully loaded: {path}\")\n",
        "\n",
        "            # Test the loaded model\n",
        "            test_image = torch.randn(1, 3, 640, 640)\n",
        "            with torch.no_grad():\n",
        "                output = loaded_model(test_image)\n",
        "                print(f\"   Test output shape: {output[0].shape if isinstance(output, tuple) else output.shape}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load {path}: {e}\")\n",
        "    else:\n",
        "        print(f\"‚ùå File not found: {path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}